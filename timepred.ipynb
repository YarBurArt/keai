{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import copy, torch, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_forecasting as pf\n",
        "from pf.data import GroupNormalizer\n",
        "from pl.loggers import TensorBoardLogger\n",
        "\n",
        "from pf.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
        "from pl.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pf import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pf.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from pytorch_forecasting.data.examples import get_stallion_data\n",
        "\n",
        "data = get_stallion_data()\n",
        "\n",
        "# add time index\n",
        "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
        "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
        "\n",
        "# add additional features\n",
        "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
        "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
        "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
        "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
        "\n",
        "\n",
        "special_days = [ \"easter_day\", \"good_friday\", \"new_year\", \"christmas\", \"labor_day\", \"independence_day\", \"revolution_day_memorial\", \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\", \"beer_capital\", \"music_fest\",]\n",
        "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
        "data.sample(6, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create dataset and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "max_prediction_length = 6\n",
        "max_encoder_length = 24\n",
        "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
        "\n",
        "training = TimeSeriesDataSet( data[lambda x: x.time_idx <= training_cutoff], time_idx=\"time_idx\", target=\"volume\", group_ids=[\"agency\", \"sku\"],\n",
        "    min_encoder_length=max_encoder_length // 2,  max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=1, max_prediction_length=max_prediction_length,\n",
        "    static_categoricals=[\"agency\", \"sku\"], static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
        "    time_varying_known_categoricals=[\"special_days\", \"month\"], variable_groups={\"special_days\": special_days},  \n",
        "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"], time_varying_unknown_categoricals=[],\n",
        "    time_varying_unknown_reals=[\"volume\", \"log_volume\", \"industry_volume\", \"soda_volume\", \"avg_max_temp\", \"avg_volume_by_agency\", \"avg_volume_by_sku\", ],\n",
        "    target_normalizer=GroupNormalizer( groups=[\"agency\", \"sku\"], transformation=\"softplus\"),  \n",
        "    add_relative_time_idx=True, add_target_scales=True, add_encoder_length=True,)\n",
        "\n",
        "\n",
        "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
        "\n",
        "\n",
        "batch_size = 64  \n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 8, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "293.0088195800781"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
        "baseline_predictions = Baseline().predict(val_dataloader)\n",
        "(actuals - baseline_predictions).abs().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Find optimal learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in network: 29.7k\n"
          ]
        }
      ],
      "source": [
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(gpus=0, gradient_clip_val=0.1,)\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(training, learning_rate=0.05, hidden_size=16, attention_head_size=1, dropout=0.2, hidden_continuous_size=8, output_size=7, loss=QuantileLoss(), reduce_on_plateau_patience=4,)\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# find optimal learning rate\n",
        "res = trainer.tuner.lr_find(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, max_lr=10.0, min_lr=1e-6,)\n",
        "print(f\"suggested learning rate: {res.suggestion()}\")\n",
        "res.plot(show=True, suggest=True).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in network: 29.7k\n"
          ]
        }
      ],
      "source": [
        "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  \n",
        "logger = TensorBoardLogger(\"lightning_logs\")\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=50, gpus=0, enable_model_summary=True, gradient_clip_val=0.1, limit_train_batches=16, callbacks=[lr_logger, early_stop_callback], logger=logger,)\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(training, learning_rate=0.03, hidden_size=16, attention_head_size=1, dropout=0.1, hidden_continuous_size=8, output_size=7, loss=QuantileLoss(), log_interval=10, reduce_on_plateau_patience=4,)\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "trainer.fit(tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader,)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
